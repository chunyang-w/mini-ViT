{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention with scaled (default) dot-product matrix multiplication.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, k, q, v, scale=None, return_score=False, dropout=0.0):\n",
    "        \"\"\"\n",
    "        perform forward pass of attention layer\n",
    "        output = ( q @ k.T / sqrt(d) ) @ V\n",
    "        Args:\n",
    "            k: of size (B, Ls, dk). or (B, n_head, Ls, dk) (if multi-head calculation is involved)\n",
    "            q: of size (B, Ls, dk). or (B, n_head, Ls, dk) (if multi-head calculation is involved)\n",
    "            v: of size (B, Ls, dv). or (B, n_head, Ls, dk) (if multi-head calculation is involved)\n",
    "            scale: Defaults to None. if specified, will scale the attention score with this value before feed into softmax.\n",
    "        \"\"\"\n",
    "        # Dropout layer\n",
    "        dropout_layer = nn.Dropout(dropout)\n",
    "        # scale = 1 / sqrt(dk)\n",
    "        scale = scale if scale is not None else 1 / math.sqrt(k.shape[-1])\n",
    "        # atten_score: shape of (Ls, Ls), where Ls is the length of source/input sequence\n",
    "        atten_score = scale * (\n",
    "            q @ k.transpose(-1, -2)\n",
    "        )\n",
    "        atten_score = torch.nn.functional.softmax(atten_score, dim=-1)\n",
    "        # Apply dropout\n",
    "        atten_score = dropout_layer(atten_score)\n",
    "        # attention: shape of (Ls, dv)\n",
    "        attention = atten_score @ v\n",
    "        return attention if not return_score else (attention, atten_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test attention layer\n",
    "\n",
    "atten_layer = Attention()\n",
    "\n",
    "bs = 16\n",
    "n_head = 8\n",
    "dk = 512\n",
    "dv = 64\n",
    "Ls = 10\n",
    "\n",
    "k = torch.randn((bs, Ls, dk))\n",
    "q = torch.randn((bs, Ls, dk))\n",
    "v = torch.randn((bs, Ls, dv))\n",
    "\n",
    "attn = atten_layer(k, q, v)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test attention layer, multi-head version\n",
    "\n",
    "atten_layer = Attention()\n",
    "\n",
    "bs = 16\n",
    "n_head = 8\n",
    "dk = 512\n",
    "dv = 64\n",
    "Ls = 10\n",
    "\n",
    "k = torch.randn((bs, n_head, Ls,  dk))\n",
    "q = torch.randn((bs, n_head, Ls, dk))\n",
    "v = torch.randn((bs, n_head, Ls, dv))\n",
    "\n",
    "attn = atten_layer(k, q, v)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Self-Attention implementation:\n",
    "\n",
    "    1. Use Linear Layer to project q, k, v to hidden dimension.\n",
    "    2. Split those projected q, k, v tensors into smaller chunks. (That's why hidden dimension size should be divisible by num_head)\n",
    "    3. Apply attention layer to those mini-chunks. (This is the multihead part).\n",
    "    4. Concatenate those processed mini-chunks, return this as the final output.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, n_head=8, bias=False, return_atten=False, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_emb (int): The dimension of token embedding.\n",
    "            num_head (int): The number of heads used in the attention mechanism.\n",
    "            return_atten (bool): Whether or not to return attention score.\n",
    "        \"\"\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert n_emb % n_head == 0\n",
    "        self.dropout = dropout\n",
    "        self.n_emb = n_emb\n",
    "        self.n_head = n_head\n",
    "        self.return_atten = return_atten\n",
    "        # The number of small chunk, head_dim:\n",
    "        self.head_dim = n_emb // n_head\n",
    "        # Matrix (Linear layers are essentially Matrix plus a bais) used to project q, k and v\n",
    "        self.q_proj = nn.Linear(n_emb, n_emb, bias=bias)\n",
    "        self.k_proj = nn.Linear(n_emb, n_emb, bias=bias)\n",
    "        self.v_proj = nn.Linear(n_emb, n_emb, bias=bias)\n",
    "        # Matrix used to project the output into\n",
    "        self.out_proj = nn.Linear(n_emb, n_emb, bias=bias)\n",
    "        # Dropout layer after attention computation:\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Forward Pass of Multihead attention block:\n",
    "\n",
    "        Args:\n",
    "            q, k, v (torch.Tensor): of shape (batch_size, seq_length, n_emb).\\\n",
    "                The embedding dim of q, k, v is the same (equal to n_emb)\\\n",
    "                this is a simplyfied implementation. In more complexed cases\\\n",
    "                where other type of attention (e.g. cross-attention) is needed,\\\n",
    "                this version may not suffice the need.\n",
    "        \"\"\"\n",
    "        bs, seq_len, n_emb = q.shape\n",
    "        assert n_emb == self.n_emb  # integrity double-check, you can never be too cautious!\n",
    "        # First use 3 Matrix (Linear Layer) to project q, k, v.\n",
    "        q_p = self.q_proj(q)\n",
    "        k_p = self.k_proj(k)\n",
    "        v_p = self.v_proj(v)\n",
    "\n",
    "        # After projection, q_p, k_p, and v_p has the same shape: (batch_size, seq_length, n_emb)\n",
    "        # split them into smaller chunks (this is the multihead part!)\n",
    "        q_p = q_p.view(bs, seq_len, self.n_head, self.head_dim)\n",
    "        k_p = k_p.view(bs, seq_len, self.n_head, self.head_dim)\n",
    "        v_p = v_p.view(bs, seq_len, self.n_head, self.head_dim)\n",
    "\n",
    "        # Then permute the dimension (batch_size, seq_length, n_head, head_dim) -> (batch_size, n_head, seq_length, head_dim)\n",
    "        # Because we do not want n_head dimension be involved in the attention calculation\n",
    "        #   i.e. each data along n_head dimension should be treated as a individual seuqence.\n",
    "        #   i.e. this is the meaning of multi-head processing.\n",
    "        \n",
    "        q_p = q_p.transpose(1, 2)\n",
    "        k_p = k_p.transpose(1, 2)\n",
    "        v_p = v_p.transpose(1, 2)\n",
    "\n",
    "        # The attention part!\n",
    "        attention_layer = Attention()\n",
    "        # Perform attention calculation. the output size should be (batch_size, n_head, seq_length, head_dim)\n",
    "        atten = attention_layer(q_p, k_p, v_p, dropout=self.dropout)\n",
    "        if self.return_atten:\n",
    "            atten_score = attention_layer(q_p, k_p, v_p, return_score=True)\n",
    "        # reshape the attention ouput, back to shape of (batch_size, n_head, seq_length, head_dim)\n",
    "        # The .contiguos() method is used to address incontiguous memory introduced by transpose methods\n",
    "        #   P.S. the .view() method won't work without this operation.\n",
    "        atten = atten.transpose(1, 2).contiguous().view(bs, seq_len, n_emb)\n",
    "        # going through the last layer: another linear transformation:\n",
    "        out = self.out_proj(atten)\n",
    "        out = self.dropout_layer(out)\n",
    "        return out if not self.return_atten else (out, atten_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([16, 10, 512])\n",
      "output shape torch.Size([16, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "bs = 16\n",
    "n_head = 8\n",
    "n_emb = 512\n",
    "dv = 64\n",
    "Ls = 10\n",
    "\n",
    "multihead_atten_layer = MultiheadAttention(n_emb=n_emb, n_head=8)\n",
    "\n",
    "x = torch.randn((bs, Ls,  n_emb))\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "out = multihead_atten_layer(x,x,x)\n",
    "print(\"output shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Implementation.\n",
    "\n",
    "    This layer will stack several multi-head attention layer with Linear Projection Layer \\\n",
    "        with residual connection and layer norm.\n",
    "    The input should be of shape (batch_size, seq_length, n_in)\n",
    "    The ouput will be of shape   (batch_size, seq_length, n_out)\n",
    "\n",
    "    Args:\n",
    "        n_in        (int):  Input dimension.\n",
    "        n_out       (int):  Output dimension.\n",
    "        n_model     (int):  Model embedding dimension.\n",
    "        num_layer   (int):  Number of transformer blocks will be stacked together.\n",
    "        bias        (bool): Whether or bias will be added to linear projection or not.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out, n_model, num_layer=6, bias=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Parameters setup\n",
    "        self.n_model = n_model\n",
    "        self.num_layer = num_layer\n",
    "        self.bias=bias\n",
    "        # layer lists: each transfomer block layer has: \n",
    "        #   + 1 multi-head attention layer\n",
    "        #   + 1 linear layer\n",
    "        #   + 2 layernorm layer\n",
    "        self.linear_layers = [nn.Linear(n_model, n_model) for i in range(num_layer)]\n",
    "        self.atten_layers = [MultiheadAttention(n_model) for i in range(num_layer)]\n",
    "        self.layernorm1 = [nn.LayerNorm(n_model) for i in range(num_layer)]\n",
    "        self.layernorm2 = [nn.LayerNorm(n_model) for i in range(num_layer)]\n",
    "        # Activation func:\n",
    "        self.act = nn.GELU()\n",
    "        # input & output projection\n",
    "        self.linear_in = nn.Linear(n_in, n_model)\n",
    "        self.linear_out = nn.Linear(n_model, n_out)\n",
    "        \n",
    "\n",
    "    def transformer_block(self, x, layer_idx):\n",
    "        \"\"\"\n",
    "        Transfomer block\n",
    "\n",
    "        input is of shape (batch_size, seq_length, n_model)\n",
    "        \"\"\"\n",
    "        atten_layer = self.atten_layers[layer_idx]\n",
    "        layernorm1 = self.layernorm1[layer_idx]\n",
    "        linear_layer = self.linear_layers[layer_idx]\n",
    "        layernorm2 = self.layernorm2[layer_idx]\n",
    "\n",
    "        # First go thru attention layer:\n",
    "        residual_atten = atten_layer(x, x, x)  # Q, K, V are the same - self-attention part\n",
    "        x = x + residual_atten                 # Residual connection\n",
    "        x = layernorm1(x)\n",
    "        # Then go thru linear layer:\n",
    "        residual_linear = linear_layer(x)\n",
    "        x = self.act(x)\n",
    "        x = x + residual_linear\n",
    "        x = layernorm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_in(x)\n",
    "        for idx in range(self.num_layer):\n",
    "            x = self.transformer_block(x, idx)\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([16, 10, 512])\n",
      "output shape torch.Size([16, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test transformer\n",
    "\n",
    "bs = 16\n",
    "n_head = 8\n",
    "n_in = 512\n",
    "n_model = 512\n",
    "n_out = 512\n",
    "dv = 64\n",
    "Ls = 10\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_in = n_in,\n",
    "    n_out = n_out,\n",
    "    n_model = n_model,\n",
    ")\n",
    "\n",
    "x = torch.randn((bs, Ls,  n_in))\n",
    "print(\"input shape: \", x.shape)\n",
    "\n",
    "out = transformer(x)\n",
    "print(\"output shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    A Simple ViT implementation.\n",
    "    The input is of shape: (batch_size, channel, H, W)\n",
    "    1. Patch the image into patches.\n",
    "    2. Compute Patch embedding (batch_size, seq_len, n_emb)\n",
    "    3. Attach class embedding.\n",
    "    4. Attach positional embedding.\n",
    "    5. Go thru Transformer.\n",
    "    6. Linear Projection layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_h, img_w, num_class, patch_size, dim, num_channel=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_h, img_w (int):         Height & Width of the input image.\n",
    "            num_class    (int):         Number of total classes in the dataset.\n",
    "            patch_size   (int):         The size of the patch window.\n",
    "            dim          (int):         The dimension of which the model operates on.\n",
    "            num_channel  (int):         The number of channels of the input image.\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__()\n",
    "        assert (img_h % patch_size == 0 and img_w % patch_size == 0)  # assert whether or not the image is patchable\n",
    "        patch_h, patch_w = img_h // patch_size, img_w // patch_size\n",
    "        patch_dim = patch_size * patch_size * num_channel\n",
    "        num_patch = patch_h * patch_w\n",
    "        self.to_patch_embedding = nn.Sequential(                      # patch embedding layer\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        # class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        # Embeddings\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, num_patch+1, dim))\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = Transformer(dim, dim, dim, num_layer=1)\n",
    "\n",
    "        # Final Linear Layer - Input should be of shape (batch_size, dim)\n",
    "        self.linear_out = nn.Linear(dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 0. init\n",
    "        # if not batched_data: (C, H, W) -> (1, C, H, W)\n",
    "        if len(x.shape) == 3:\n",
    "            x = torch.unsqueeze(x, dim=0)\n",
    "        bs, C, H, W = x.shape\n",
    "        # 1. Patrchify: (batch_size, C, H, W) -> (batch_size, num_patch, dim)\n",
    "        patch = self.to_patch_embedding(x)\n",
    "        # 2. Attach embedding - class token: (batch_size, num_patch, dim) -> (batch_size, num_patch+1, dim)\n",
    "        cls_token = repeat(self.cls_token, '1 1 d -> bs 1 d', bs=bs)\n",
    "        x = torch.cat([cls_token, patch], dim=1)\n",
    "        # 2. Attach embedding - positional embedding\n",
    "        x = x + self.pos_emb\n",
    "        # 3. go thru transformer\n",
    "        x = self.transformer(x)\n",
    "        # 4. get output class token\n",
    "        x = x[:, 0]  # of shape (batch_size, dim)\n",
    "        # 5. go thru linear classification layer:\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "bs = 64\n",
    "\n",
    "class GrayscaleToRGB(object):\n",
    "    \"\"\"Convert a 1-channel grayscale image to a 3-channel RGB image.\"\"\"\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Grayscale image to be converted to RGB.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: RGB image with three channels.\n",
    "        \"\"\"\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                ])\n",
    "\n",
    "train_set = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(img_h=28, img_w=28, num_class=10, patch_size=7, dim=512, num_channel=1)\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7907\n",
      "Epoch [2/5], Loss: 0.2551\n",
      "Epoch [3/5], Loss: 0.1876\n",
      "Epoch [4/5], Loss: 0.1625\n",
      "Epoch [5/5], Loss: 0.1454\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 95.68%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \t4, label: \t4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize(model, dataset, idx=None):\n",
    "    if idx is None:\n",
    "        idx = random.randint(0, len(dataset))\n",
    "    data, label = dataset[idx]\n",
    "    plt.imshow(data.squeeze(), cmap=\"grey\")\n",
    "    pred = model(data)\n",
    "    pred = torch.max(pred.data, 1).indices[0]\n",
    "    print(f\"Prediction: \\t{pred}, label: \\t{label}\")\n",
    "\n",
    "visualize(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
